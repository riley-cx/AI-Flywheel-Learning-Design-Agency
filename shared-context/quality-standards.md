# Quality Standards for AI Flywheel Learning Design
## Elite Learning Design Agency - Shared Knowledge File

**Version:** 1.0
**Last Updated:** October 25, 2025
**Purpose:** Define what "excellent" means for our elite learning design team
**Audience:** All 10 AI agents, human collaborators, external partners
**Context:** This team is top 1% - IDEOU and Hyper Island want them, but we have them

---

## TL;DR - Quality at AI Flywheel

**Who We Are:** An elite learning design agency in the top 1% of the profession

**What Sets Us Apart:** We don't just create courses. We create transformation that sticks. 85%+ completion rates vs 65% industry average.

**Our Standard:** If it doesn't survive Monday morning, it's not ready to ship.

**Core Quality Metric:** Would a designer who takes this course still be using it 6 months later? If no, we haven't done our job.

---

## The AI Flywheel Quality Philosophy

### **Principle 1: Transformation Over Information**

**What This Means:**
Learning design isn't about delivering content. It's about changing behaviour at scale.

**Quality Standard:**
- ‚ùå FAIL: "Students complete modules and pass quizzes"
- ‚úÖ PASS: "Students change how they work on Monday morning"
- üèÜ ELITE: "Students are still using these practices 6 months later"

**How We Measure:**
- Completion rates (target: 85%+)
- Practice adoption (% using frameworks in real work)
- Retention (% still active 6 months post-course)
- Impact (measurable changes in their design practice)

---

### **Principle 2: Habits Over Knowledge**

**What This Means:**
Knowledge fades. Habits stick. We design for habit formation, not information transfer.

**Quality Standard:**
- ‚ùå FAIL: "Here's what you need to know about AI bias"
- ‚úÖ PASS: "Here are 5 questions to ask in every design review"
- üèÜ ELITE: "After 4 weeks, this ritual becomes automatic"

**Design Implications:**
- Every module includes daily/weekly rituals
- Scaffolded practice (small ‚Üí larger ‚Üí embedded)
- Environmental cues (Slack reminders, desktop checklists)
- Peer accountability (share progress, get feedback)

---

### **Principle 3: Practical Over Theoretical**

**What This Means:**
Designers don't need philosophical debates. They need Monday morning answers.

**Quality Standard:**
- ‚ùå FAIL: "Kant's categorical imperative suggests..."
- ‚úÖ PASS: "Before pasting that into ChatGPT, ask these 3 questions"
- üèÜ ELITE: "Here's the exact prompt template that keeps client data private"

**The Monday Morning Test:**
Every piece of content must answer: **"What do I do tomorrow with THIS specific project?"**

If a designer can't apply it immediately to real work, it fails our quality bar.

---

### **Principle 4: Complexity Honoured, Not Hidden**

**What This Means:**
AI ethics is genuinely hard. We don't pretend it's easy. We make hard things learnable.

**Quality Standard:**
- ‚ùå FAIL: "Just follow these 10 simple steps!"
- ‚úÖ PASS: "This is hard. Here's how to break it into learnable pieces."
- üèÜ ELITE: "We scaffold from mental models ‚Üí knowledge ‚Üí skills ‚Üí habits"

**How We Honour Complexity:**
- Acknowledge difficulty upfront ("This is genuinely hard")
- Explain WHY it's complex (not just HOW to do it)
- Provide decision frameworks (not just rules)
- Build in reflection time (absorption, not speed)

---

### **Principle 5: Community-Centred, Not Content-Centred**

**What This Means:**
Transformation happens in community, not isolation. Peer learning > expert lectures.

**Quality Standard:**
- ‚ùå FAIL: "Watch videos, do exercises, get certificate"
- ‚úÖ PASS: "Learn together in cohorts, share struggles, support peers"
- üèÜ ELITE: "Community becomes the support system that outlasts the course"

**Design Requirements:**
- Cohort-based (never self-paced only)
- Peer critique built into every module
- Slack community as primary learning space
- Office hours for real-time troubleshooting
- Post-course community access (alumni network)

---

## Quality Standards by Content Type

### **1. Course Modules (Educational Content)**

**Structural Quality Standards:**

**Opening (First 60 seconds):**
- ‚úÖ Hook: Why this matters to their work RIGHT NOW
- ‚úÖ Stakes: What happens if they get this wrong
- ‚úÖ Promise: What they'll be able to do after this module
- ‚ùå NO: Generic intros, corporate training voice

**Body (Learning Content):**
- ‚úÖ Mental model FIRST (how to think about this)
- ‚úÖ Knowledge SECOND (what you need to know)
- ‚úÖ Skills THIRD (how to do it)
- ‚úÖ Habits FOURTH (rituals to embed it)
- ‚úÖ Real examples from design practice (not abstract case studies)
- ‚úÖ Before/After comparisons (show the transformation)
- ‚ùå NO: Theory without application, jargon without definition

**Practice (Application):**
- ‚úÖ Tiny first step (10 minutes, real project)
- ‚úÖ Scaffolded exercises (progressively complex)
- ‚úÖ Peer review built in (share work, get feedback)
- ‚úÖ Reflection prompts (what did you notice?)
- ‚ùå NO: Fake scenarios, made-up projects, solo work only

**Closing (Last 60 seconds):**
- ‚úÖ One ritual to take into Monday morning
- ‚úÖ How to know it's working
- ‚úÖ What to do if stuck
- ‚úÖ Preview next module (build momentum)
- ‚ùå NO: "Great job, see you next time"

**Quality Checklist for Every Module:**
- [ ] Passes The Monday Morning Test (immediately applicable)
- [ ] Uses UK English spelling throughout
- [ ] No AI-generated voice markers (em-dashes, fake enthusiasm)
- [ ] Defines all jargon in human terms first
- [ ] Includes at least 2 real design scenarios (not abstract)
- [ ] Builds towards a specific habit/ritual
- [ ] Contains peer interaction element
- [ ] Runtime appropriate (15-25 min video, 45-60 min total including exercises)
- [ ] Passes Four Voice Tests (Warmth, Vulnerability, Anti-Corporate, Practicality)

---

### **2. Student Support (Troubleshooting, Encouragement)**

**Quality Standards for Support Interactions:**

**Response Time:**
- üèÜ ELITE: < 2 hours during business hours
- ‚úÖ GOOD: < 24 hours
- ‚ùå FAIL: > 48 hours

**Response Quality:**
- ‚úÖ Validates struggle ("This is genuinely hard")
- ‚úÖ Offers specific next step (not "review the module")
- ‚úÖ Connects to peer support ("Share in Slack - others feeling this")
- ‚úÖ Uses warm, peer-to-peer tone (never corporate)
- ‚ùå NO: Generic responses, copy-paste answers, "contact support"

**Support Escalation:**
- Level 1: Community Manager handles (90% of cases)
- Level 2: Learning Designer clarifies content (8% of cases)
- Level 3: Riley personal response (2% - crisis, major issue)

**Quality Checklist for Support:**
- [ ] Responds within SLA (< 24 hours)
- [ ] Validates emotion/struggle
- [ ] Provides actionable next step
- [ ] Connects to community
- [ ] Uses brand voice (warm, vulnerable, practical)
- [ ] Follows up if no resolution within 48 hours

---

### **3. Assessments & Validation**

**Philosophy:** We don't test knowledge. We validate practice.

**Quality Standards:**

**Instead of Quizzes:**
- ‚úÖ Real project application ("Use this framework on YOUR current project")
- ‚úÖ Peer review ("Share your work, critique 2 others")
- ‚úÖ Reflection ("What changed in how you think about this?")
- ‚ùå NO: Multiple choice, memorisation tests, right/wrong answers

**Validation Criteria:**
- Did they apply it to real work? (not hypothetical)
- Can they explain their thinking? (not just repeat back)
- Did they help a peer? (community contribution)
- Are they building the habit? (repeated practice)

**Quality Checklist for Assessments:**
- [ ] Based on real design work (not made-up scenarios)
- [ ] Requires reflection, not memorisation
- [ ] Includes peer interaction
- [ ] Validates practice, not just knowledge
- [ ] Provides actionable feedback (not just "correct/incorrect")

---

### **4. Community Management**

**Philosophy:** The community IS the product. Safe, supportive, high-signal.

**Quality Standards:**

**Community Health Metrics:**
- üèÜ ELITE: 80%+ of cohort active weekly
- ‚úÖ GOOD: 60%+ active weekly
- ‚ùå FAIL: < 40% active weekly

**Signal-to-Noise Ratio:**
- ‚úÖ High-signal: Real struggles, work-in-progress, peer help
- ‚ùå Noise: Self-promotion, off-topic, negativity without purpose

**Moderation Standards:**
- Response to every introduction post (within 24h)
- Celebrate small wins publicly
- Connect struggling members to peers who've solved similar issues
- Escalate to Learning Designer if 3+ people ask same question (content gap)
- Remove spam/self-promotion immediately
- Handle conflict privately (never public call-outs)

**Quality Checklist for Community:**
- [ ] Every new member welcomed within 24h
- [ ] At least 3 meaningful interactions per week from team
- [ ] Struggling members connected to peer support
- [ ] Wins celebrated (without toxic positivity)
- [ ] Safe space maintained (no judgement, no "stupid questions")

---

## Quality Standards by Learning Design Element

### **Mental Models (How to Think)**

**Purpose:** Give designers a new lens for seeing problems

**Quality Standard:**
- ‚úÖ Simple enough to remember
- ‚úÖ Complex enough to be useful
- ‚úÖ Applies across contexts (not one-off)
- ‚úÖ Changes how they see their work

**Examples of Elite Mental Models:**
- "The Technical Training Chasm" (why courses don't change behaviour)
- "Best Intentions ‚â† No Harm" (the gap we fill)
- "Goldilocks Formula for Privacy" (not too much, not too little)

**Quality Test:**
Can they explain this mental model to a colleague in 60 seconds? If no, it's too complex.

---

### **Frameworks (How to Decide)**

**Purpose:** Give designers tools for navigating complexity

**Quality Standard:**
- ‚úÖ Actionable steps (not abstract principles)
- ‚úÖ Decision criteria (how to choose)
- ‚úÖ Examples of use (before/after)
- ‚úÖ Edge cases addressed (when it doesn't apply)

**Framework Design Requirements:**
- Maximum 5-7 steps (more = cognitive overload)
- Each step has clear output (not vague "consider...")
- Real examples from design practice
- Common mistakes identified
- When NOT to use this framework

**Quality Test:**
Can they use this framework on a real project within 24 hours? If no, it's too abstract.

---

### **Rituals (How to Embed)**

**Purpose:** Turn knowledge into automatic practice

**Quality Standard:**
- ‚úÖ Specific trigger (when to do this)
- ‚úÖ Tiny first step (< 5 minutes)
- ‚úÖ Observable outcome (you'll know it worked)
- ‚úÖ Peer accountability (share progress)

**Ritual Design Pattern:**
```
WHEN [trigger event]
DO [specific tiny action]
EXPECT [observable outcome]
SHARE [with community]
```

**Example Elite Ritual:**
```
WHEN you're about to paste text into ChatGPT
DO ask "Does this contain names, locations, health info?"
EXPECT 5 seconds of pause before pasting
SHARE risky catches in #wins Slack channel
```

**Quality Test:**
Are they still doing this ritual 4 weeks later? If no, the trigger wasn't clear or the action was too complex.

---

### **Examples & Case Studies**

**Purpose:** Make abstract concrete through real design work

**Quality Standard:**
- ‚úÖ Real design scenarios (healthcare, finance, education, etc.)
- ‚úÖ Specific details (not "imagine you're designing an app")
- ‚úÖ Before/After comparison (what changed)
- ‚úÖ Messy reality honoured (not sanitised success stories)

**What Makes an Example "Elite":**
- Grounded in real design practice (not made-up)
- Shows the thinking process (not just the outcome)
- Includes what didn't work (failures teach)
- Connects to framework/ritual being taught
- Appropriate complexity (not oversimplified)

**Quality Test:**
Does this example feel like their actual work? If it feels academic or made-up, it fails.

---

## The AI Flywheel Quality Rubric

Every piece of content must score 4+ on each dimension (5-point scale):

### **1. Immediate Applicability (Monday Morning Test)**
- 1 - Theoretical only, no practical application
- 2 - Some practical elements, but mostly abstract
- 3 - Applicable but requires significant adaptation
- 4 - Directly applicable to real design work
- 5 - Can use THIS WEEK on current project

### **2. Habit Formation (Stickiness)**
- 1 - Pure information, no behaviour change
- 2 - Suggests actions but no ritual/trigger
- 3 - Clear action, but one-off (not repeatable)
- 4 - Ritual defined with trigger and accountability
- 5 - Ritual becomes automatic within 4 weeks

### **3. Community Integration (Peer Learning)**
- 1 - Solo learning only
- 2 - Optional peer interaction
- 3 - Peer interaction suggested
- 4 - Peer interaction required
- 5 - Peer learning is primary mechanism

### **4. Complexity Honoured (Not Hidden)**
- 1 - Oversimplified to point of inaccuracy
- 2 - Acknowledges complexity but doesn't address it
- 3 - Breaks down complexity adequately
- 4 - Scaffolds from simple ‚Üí complex thoughtfully
- 5 - Mental models make complexity learnable

### **5. Voice Quality (Brand Alignment)**
- 1 - Corporate/cold/jargon-heavy
- 2 - Attempts warmth but inconsistent
- 3 - Mostly on-brand with minor issues
- 4 - Consistently warm, vulnerable, practical
- 5 - Indistinguishable from Riley's personal voice

**Minimum Quality Bar:** 20/25 (4+ on each dimension)

**Elite Standard:** 23+/25 (4.6+ average)

---

## Quality Assurance Process

### **Pre-Production Quality Gates**

**Gate 1: Concept Validation**
- Chief Learning Strategist approves concept
- Aligns with strategic objectives
- Fills identified gap in curriculum
- Appropriate for target cohort

**Gate 2: Learning Design Review**
- Learning Designer reviews outline/structure
- Validates mental model ‚Üí knowledge ‚Üí skills ‚Üí habits flow
- Confirms Monday Morning applicability
- Approves assessment/practice approach

**Gate 3: Content Draft Review**
- Quality Assurance Specialist scores against rubric (must be 20+/25)
- Behavioural Scientist validates habit formation design
- Experience Designer reviews UX/flow
- All must approve before production

**Gate 4: Brand Voice Check**
- Chief Experience Strategist reviews for voice consistency
- Runs Four Voice Tests (Warmth, Vulnerability, Anti-Corporate, Practicality)
- Checks UK English, no AI markers
- Must pass all tests before recording/production

### **Production Quality Gates**

**Gate 5: Technical Quality**
- Experience Designer checks video/audio quality
- Accessibility requirements met (captions, transcripts)
- Platform integration tested
- No technical issues

**Gate 6: Beta Testing**
- Small cohort (5-10 designers) tests module
- Completion rates tracked
- Feedback collected
- Revisions made before full launch

**Gate 7: Launch Quality**
- Community Manager monitors first 48h
- Quick wins celebrated
- Struggles identified and addressed
- Support resources added if gaps found

### **Post-Production Quality Monitoring**

**Ongoing Metrics (Weekly):**
- Completion rates (target: 85%+)
- Time-to-completion (flag if > 2x estimated)
- Support tickets (flag if > 5 on same topic)
- Community questions (flag if repeated)

**Quality Triggers for Revision:**
- Completion < 70% (content too hard or not engaging)
- Support tickets > 10 on same issue (clarity problem)
- Peer reviews show consistent confusion (concept not landing)
- 6-month retention < 60% (habit not sticking)

---

## Elite Learning Design Patterns

These patterns represent top 1% learning design. IDEOU and Hyper Island use these. We invented some of them.

### **Pattern 1: The Goldilocks Scaffolding**

**Concept:** Not too easy (boring), not too hard (overwhelming), just right (flow state)

**Implementation:**
- Start with familiar (designer mental models)
- Add one new concept at a time
- Practice before complexity increases
- Peer support when challenge rises

**Quality Indicator:**
Students report feeling "stretched but supported" (not overwhelmed or bored)

---

### **Pattern 2: The Struggle Share Ritual**

**Concept:** Normalise struggle, extract collective wisdom

**Implementation:**
- Every module includes "What's Hard?" prompt
- Students share real struggles in Slack
- Peers respond with "I felt this too" + what helped
- Learning Designer synthesises patterns for FAQ

**Quality Indicator:**
80%+ of cohort shares a struggle at least once (psychological safety achieved)

---

### **Pattern 3: The Real Project Guarantee**

**Concept:** Every exercise uses student's actual current work (no made-up scenarios)

**Implementation:**
- First exercise: "Pick ONE project you're working on this week"
- All subsequent practice uses THAT project
- Peer critique based on real context
- Outcome: They ship better work, not just complete exercises

**Quality Indicator:**
Students report shipping higher-quality work on real projects (measured via post-course survey)

---

### **Pattern 4: The Tiny Habit Trigger**

**Concept:** Behaviour change through environmental cues, not willpower

**Implementation:**
- Every framework includes specific trigger ("WHEN you open Figma...")
- Tiny action (< 30 seconds)
- Immediate feedback (you'll notice X)
- Slack reminder bot reinforces trigger

**Quality Indicator:**
60%+ report habit became automatic within 4 weeks

---

### **Pattern 5: The Peer Expert Flip**

**Concept:** Students become teachers (deepest learning)

**Implementation:**
- Week 3: Each student teaches ONE concept to peer
- Week 4: Students create mini-case study from their work
- Week 5: Students facilitate peer critique session
- Outcome: Community becomes self-sustaining

**Quality Indicator:**
Alumni stay active in community 6+ months post-course (teaching new cohorts)

---

## What "Elite" Looks Like in Practice

### **Elite Course Design (Top 1%):**
- 85%+ completion (vs 65% industry average)
- 6-month retention: 70%+ still using practices
- NPS > 70 (vs 30-40 industry average)
- Community stays active post-course (not ghost town)
- Students teach peers (knowledge multiplies)

### **Elite Support:**
- < 2 hour response time (vs 24-48h industry)
- Peer-to-peer help > expert help (community solves)
- Proactive (we spot struggles before student asks)
- Personal (Riley responds to 2% most critical issues)

### **Elite Community:**
- 80%+ weekly active (vs 20-30% industry)
- High-signal (real work, real struggles, real help)
- Psychologically safe (no stupid questions, no judgement)
- Alumni return to mentor new cohorts

---

## Quality Standards Summary

**We don't ship content that:**
- ‚ùå Won't survive Monday morning
- ‚ùå Requires memorisation without understanding
- ‚ùå Works in theory but not practice
- ‚ùå Ignores the messy reality of design work
- ‚ùå Sounds like corporate training or coding bootcamp
- ‚ùå Can't be applied to real projects immediately
- ‚ùå Doesn't build towards habit formation

**We only ship content that:**
- ‚úÖ Changes behaviour on Monday morning
- ‚úÖ Builds habits that stick 6 months later
- ‚úÖ Honours complexity while making it learnable
- ‚úÖ Connects to real design practice
- ‚úÖ Sounds like Riley talking to a peer
- ‚úÖ Passes the Quality Rubric (20+/25 minimum)
- ‚úÖ Makes the community stronger

---

## File Metadata

**File Type:** Shared Knowledge for AI Agents
**Update Frequency:** Quarterly (or when quality bar evolves)
**Owner:** Chief Learning Strategist + Learning Designer
**Related Files:**
- `strategic-objectives.md` (what we're focusing on)
- `brand-voice.md` (how we communicate)
- `student-personas.md` (who we're designing for)
- `success-metrics.md` (how we measure impact)

---

## Changelog

**v1.0 - October 25, 2025**
- Initial version created
- Five core quality principles established
- Quality standards defined by content type
- Elite learning design patterns documented
- Quality rubric created (5-point scale, 5 dimensions)
- QA process gates defined
- Elite benchmarks set (85%+ completion, 70%+ 6-month retention)

---

**END OF QUALITY STANDARDS GUIDE**
