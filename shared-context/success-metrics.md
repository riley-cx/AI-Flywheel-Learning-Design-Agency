# Success Metrics for AI Flywheel
## Elite Learning Design Agency - Shared Knowledge File

**Version:** 1.0
**Last Updated:** October 25, 2025
**Purpose:** Define how we measure if we're actually creating transformation (not just delivering content)
**Audience:** All 10 AI agents, especially Data Analyst, Chief Learning Strategist, QA Specialist
**Context:** We're top 1%. We measure what matters: behaviour change that sticks, not vanity metrics.

---

## TL;DR - How We Measure Success

**Not This:** Course completion rates, quiz scores, certificates issued

**But This:** Are designers still using these practices 6 months later? Are they safer? Better?

**Core Philosophy:** Transformation over information. Habits over knowledge. Impact over engagement.

**Primary Metric:** 6-month retention (% of students still actively using frameworks in their work)

---

## The AI Flywheel Metrics Philosophy

### **Principle: Measure Outcomes, Not Outputs**

**Outputs (Vanity Metrics):**
- ❌ Number of videos watched
- ❌ Modules completed
- ❌ Quiz scores
- ❌ Time on platform
- ❌ Certificates downloaded

**Outcomes (Transformation Metrics):**
- ✅ Behaviour changed on Monday morning
- ✅ Still using practices 6 months later
- ✅ Prevented harm (privacy breach avoided)
- ✅ Team transformation (leader taught their team)
- ✅ Community sustained (alumni helping new cohorts)

**Why This Matters:**
- We can have 100% completion rates and zero impact
- We can have perfect quiz scores and zero behaviour change
- Elite learning design optimises for outcomes, not outputs

---

## The Hierarchy of Success Metrics

### **Tier 1: Business Health (Can We Sustain?)**

These metrics tell us if the business model works.

**Revenue Metrics:**
- **Target**: $520K revenue in Q4 2024
- **Growth**: +15% quarter-over-quarter
- **CAC (Customer Acquisition Cost)**: < $400 per student
- **LTV (Lifetime Value)**: > $1,200 per student
- **LTV:CAC Ratio**: > 3:1

**Why These Matter:**
- If we can't sustain financially, we can't help anyone
- Low CAC + High LTV = word-of-mouth is working (students become advocates)
- 3:1 ratio = healthy, profitable, sustainable

**Data Sources:**
- Stripe (revenue)
- Marketing analytics (CAC)
- Cohort analysis (LTV, repeat purchases, referrals)

---

### **Tier 2: Learning Transformation (Are We Creating Change?)**

These metrics tell us if we're actually transforming how designers work.

#### **Metric 1: Course Completion Rate**

**Target**: 85%+ (vs 65% industry average)

**What It Means:**
- Students finish because it's valuable, not just because they paid
- Content is engaging and practical
- Support system is working

**How We Measure:**
- Completed all core modules + final project
- Tracked per cohort (not all-time average)
- Segmented by persona (Sarah vs Marcus vs Priya)

**Red Flags:**
- < 70%: Content too hard, not engaging, or poor support
- Major drop-off at specific module: Content clarity issue
- Specific persona struggling: Need persona-specific support

**Data Sources:**
- Learning platform analytics
- Cohort tracking dashboard

---

#### **Metric 2: Practice Adoption Rate**

**Target**: 75%+ using frameworks in real work within 2 weeks

**What It Means:**
- Monday Morning Test is working
- Content is practical, not theoretical
- Students see immediate value

**How We Measure:**
- Self-reported: "Have you used [Framework X] in your work this week?"
- Evidence-based: Share work examples in Slack
- Peer validation: Other designers confirm it's real work

**Measurement Points:**
- Week 2 (immediate application)
- Week 4 (habit forming)
- Week 8 (sustained practice)

**Red Flags:**
- < 50%: Frameworks too abstract or not applicable
- High reported use but no evidence: Social desirability bias (they're lying to please us)
- Persona-specific patterns: Sarah uses but Priya doesn't = need scaffolding

**Data Sources:**
- Weekly check-in surveys
- Slack channel analysis (work shared)
- Peer review submissions

---

#### **Metric 3: 6-Month Retention**

**Target**: 70%+ still using practices 6 months post-course

**What It Means:**
- Habits stuck (not just knowledge acquired)
- Behaviour change was real
- Transformation > information

**How We Measure:**
- 6-month post-course survey: "Which frameworks are you still using?"
- Evidence required: "Share an example from this month"
- Alumni Slack activity: Still helping new cohorts

**This Is Our Primary Success Metric**
- Everything else is leading indicator
- This is the outcome we optimize for
- Elite = 70%+, Good = 50-69%, Fail = < 50%

**Red Flags:**
- < 50%: Knowledge faded, habits didn't stick, content was theory
- Specific framework dropout: That framework wasn't practical enough
- Persona-specific retention: Leaders (Marcus) retain better than solo designers (Priya) = need ongoing community support

**Data Sources:**
- 6-month alumni survey
- Alumni Slack activity logs
- Case study interviews

---

#### **Metric 4: Harm Prevention**

**Target**: 90%+ report avoiding privacy/bias risks they would have missed before

**What It Means:**
- We're achieving our mission (prevent harm)
- Designers are safer than before
- The "3am anxiety" is productive, not paralysing

**How We Measure:**
- Self-reported: "Caught a privacy risk you would have missed?"
- Specific examples: "What was the risk? How did you handle it?"
- Team impact: "Taught your team this practice?"

**Stories We Track:**
- "Stopped before pasting client data into ChatGPT"
- "Caught bias in AI feature during design review"
- "Pushed back on stakeholder asking for unsafe AI"
- "Created privacy-preserving alternative"

**Red Flags:**
- < 70%: Content is theoretical, not translating to real decisions
- No specific examples: Social desirability bias (want to please us)
- Anxiety increased but no action: We scared them without empowering them

**Data Sources:**
- Weekly reflection prompts
- #wins Slack channel (students share catches)
- Case study interviews

---

### **Tier 3: Community Health (Is the Ecosystem Thriving?)**

These metrics tell us if we're building a sustainable, healthy community.

#### **Metric 5: Weekly Active Community Members**

**Target**: 80%+ of cohort active weekly (vs 20-30% industry)

**What It Means:**
- Community is valuable (not ghost town)
- Peer learning is happening
- Safe space for vulnerability

**What "Active" Means:**
- Posted a question, struggle, or win
- Responded to peer's post
- Attended office hours
- Shared work for critique

**Red Flags:**
- < 60%: Community not engaging, content is solo-digestible (bad design)
- High lurking, low posting: Not psychologically safe
- Same 10 people posting: Clique forming, others excluded

**Data Sources:**
- Slack analytics (posts, replies, active users)
- Office hours attendance
- Peer review participation

---

#### **Metric 6: Peer Support Ratio**

**Target**: 70%+ of questions answered by peers (not just team)

**What It Means:**
- Community is self-sustaining
- Knowledge distributed (not centralized in Riley)
- Peer-to-peer transformation working

**How We Measure:**
- Track who answers questions in Slack
- Team answers vs peer answers
- Quality of peer answers (validated by team)

**Progression We Want to See:**
- Week 1-2: 80% team answers (students ramping up)
- Week 3-4: 50/50 team and peers
- Week 5+: 70%+ peer answers
- Post-course: 90%+ peer answers (alumni helping new cohorts)

**Red Flags:**
- Still 80% team answers in Week 5+: Community not gelling
- Low-quality peer answers: Students don't understand well enough to teach
- No alumni participation: They didn't retain knowledge or don't value community

**Data Sources:**
- Slack message analysis (who answers whom)
- Answer quality scores (team validation)
- Alumni participation rates

---

#### **Metric 7: Psychological Safety Index**

**Target**: 85%+ feel safe sharing struggles

**What It Means:**
- Vulnerability is normalised
- No judgement culture
- Learning > performing

**How We Measure:**
- Survey: "I feel comfortable sharing when I'm stuck"
- Survey: "I've seen others struggle and get support"
- Observation: Do people share real struggles or just wins?

**Evidence of Safety:**
- Students post "I don't understand X"
- Students share work-in-progress (not just polished)
- Students admit mistakes ("I did this wrong, here's what happened")
- Peers respond with "me too" and help

**Red Flags:**
- Only wins posted (no vulnerability)
- Questions are softened ("This is probably stupid but...")
- Students DM team instead of posting publicly (fear of judgement)
- Cliques forming (in-group, out-group)

**Data Sources:**
- Psychological safety survey (monthly)
- Slack sentiment analysis (tone, vulnerability markers)
- Community Manager observations

---

### **Tier 4: Student Satisfaction (Do They Love It?)**

These metrics tell us if students are happy and would recommend us.

#### **Metric 8: Net Promoter Score (NPS)**

**Target**: 70+ (vs 30-40 industry average for online courses)

**Question**: "On a scale of 0-10, how likely are you to recommend AI Flywheel to a fellow designer?"

**Segments:**
- 9-10: Promoters (we want 70%+)
- 7-8: Passives (neutral, won't hurt or help)
- 0-6: Detractors (unhappy, may damage brand)

**NPS Calculation**: % Promoters - % Detractors

**Why 70+ Is Elite:**
- Apple: ~70
- Tesla: ~90
- Most online courses: 30-40
- Mediocre courses: 10-20

**When We Measure:**
- Mid-course (Week 4): Early warning system
- End of course (Week 8): Completion satisfaction
- 6 months post: Long-term satisfaction

**Red Flags:**
- NPS < 50: Serious quality issues
- Detractors > 10%: Major problems to address
- Persona-specific issues: One persona unhappy = content mismatch

**Data Sources:**
- Post-course NPS survey
- 6-month alumni survey

---

#### **Metric 9: Course Satisfaction (CSAT)**

**Target**: 4.5/5.0 average (vs 3.8 industry)

**What We Ask:**
- "How satisfied are you with this course?"
- Scale: 1-5 (Very Dissatisfied → Very Satisfied)

**Segmented By:**
- Module satisfaction (which modules land, which don't)
- Support satisfaction (response time, quality)
- Community satisfaction (valuable or ghost town)

**Red Flags:**
- < 4.0: Major satisfaction issues
- Specific module < 3.5: Content problem, needs revision
- Support < 4.0: Response time or quality issue

**Data Sources:**
- End-of-module surveys
- Final course survey
- Support ticket sentiment analysis

---

### **Tier 5: Business Impact (Are We Growing Sustainably?)**

These metrics tell us if we're building a sustainable, growing business.

#### **Metric 10: Word-of-Mouth Referrals**

**Target**: 40%+ of new students from referrals

**What It Means:**
- Students become advocates
- Low CAC (organic growth)
- Brand strength

**How We Track:**
- "How did you hear about us?" (required field)
- Referral codes (students get credit)
- LinkedIn/Twitter mentions (tracked)

**Why 40% Is Elite:**
- Most online courses: 10-20% referrals
- Great products: 30-40% referrals
- Exceptional: 50%+ referrals

**Red Flags:**
- < 20%: Students aren't advocating (why not?)
- Declining over time: Quality dropping or market saturating
- Persona-specific: Leaders refer, mid-levels don't = need different incentives

**Data Sources:**
- Signup form (referral source)
- Referral tracking system
- Social media monitoring

---

#### **Metric 11: Repeat Purchase Rate**

**Target**: 30%+ take advanced course or renew community

**What It Means:**
- We delivered value (they want more)
- LTV increases (sustainable business)
- Product-market fit

**What Counts:**
- Advanced courses (AI Safety Deep Dive, etc.)
- Community renewal (annual alumni membership)
- Team purchases (Marcus brings his whole team)

**Red Flags:**
- < 15%: First course didn't deliver enough value
- High course 1 satisfaction but low repeat: No clear next step
- Leaders repeat but mid-levels don't: Pricing or positioning issue

**Data Sources:**
- Purchase history
- Upsell conversion rates
- Alumni renewal rates

---

## How Agents Use These Metrics

### **Data Analyst: Monitor & Alert**

**Daily Monitoring:**
- Completion rates (by cohort, by module)
- Community activity (posts, replies, attendance)
- Support tickets (volume, topic, sentiment)

**Weekly Reports:**
- Cohort health dashboard
- Red flags identified
- Recommendations for intervention

**Alerts Triggered When:**
- Completion drops below 80% (alert Learning Designer)
- Community activity below 70% (alert Community Manager)
- Specific module satisfaction < 3.5 (alert QA Specialist)
- Support tickets spike on one topic (alert Learning Designer - content gap)

---

### **Chief Learning Strategist: Strategic Decisions**

**Uses Metrics For:**
- Course roadmap (what to build next based on retention data)
- Content revision priorities (which modules need work)
- Resource allocation (where to invest time)
- Goal setting (OKRs based on current performance)

**Key Questions:**
- Are we hitting 85%+ completion? If no, why not?
- Is 6-month retention above 70%? If no, what habits aren't sticking?
- Which personas are thriving vs struggling? How do we adapt?
- What's our NPS trend? Are we getting better or worse?

---

### **Learning Designer: Content Optimization**

**Uses Metrics For:**
- Module revision priorities (satisfaction scores)
- Practice exercise effectiveness (adoption rates)
- Complexity calibration (completion patterns)
- Framework validation (6-month retention by framework)

**Red Flags to Watch:**
- Module completion drop-off (too hard? not valuable?)
- Low practice adoption (too abstract? not applicable?)
- Framework not retained at 6 months (didn't become habit)
- Persona-specific struggles (need scaffolding variants)

---

### **Community Manager: Community Health**

**Uses Metrics For:**
- Engagement interventions (who needs nudge?)
- Content curation (what topics are hot?)
- Psychological safety monitoring (is it safe to be vulnerable?)
- Alumni activation (bring back post-course community)

**Red Flags to Watch:**
- < 80% weekly active (need engagement boost)
- < 70% peer answers (community not self-sustaining)
- Psychological safety < 85% (safety interventions needed)
- Clique formation (inclusive interventions needed)

---

### **Quality Assurance Specialist: Quality Gates**

**Uses Metrics For:**
- Pre-launch quality checks (pilot cohort data)
- Post-launch monitoring (first 2 weeks critical)
- Revision triggers (when to update content)
- Standards enforcement (maintaining elite bar)

**Quality Gates:**
- Module satisfaction < 4.0 → Revise before next cohort
- Completion rate < 75% → Investigate and fix
- Practice adoption < 60% → Frameworks too abstract
- 6-month retention < 60% → Habit formation failed

---

## Success Metrics Dashboard (What We Track)

### **Real-Time (Daily):**
- Course completion rate (current cohort)
- Community posts/replies (last 24h)
- Support tickets open/resolved
- Office hours attendance

### **Weekly:**
- Practice adoption rate (self-reported + evidence)
- Peer support ratio (who's answering questions)
- Module satisfaction scores
- Cohort health index (composite score)

### **Monthly:**
- NPS (promoters vs detractors)
- CSAT (satisfaction trends)
- Psychological safety index
- Referral rate

### **Quarterly:**
- 6-month retention (alumni survey)
- Harm prevention stories (qualitative)
- Revenue vs target
- LTV:CAC ratio

---

## Target Performance (What Elite Looks Like)

### **Elite Tier (Top 1%):**
- ✅ 85%+ completion rate
- ✅ 75%+ practice adoption (Week 2)
- ✅ 70%+ 6-month retention
- ✅ 90%+ harm prevention rate
- ✅ 80%+ weekly community active
- ✅ 70+ NPS
- ✅ 4.5/5.0 CSAT
- ✅ 40%+ referral rate

### **Good Tier (Solid Performance):**
- ✅ 75-84% completion
- ✅ 60-74% practice adoption
- ✅ 60-69% 6-month retention
- ✅ 80-89% harm prevention
- ✅ 70-79% community active
- ✅ 50-69 NPS
- ✅ 4.0-4.4 CSAT
- ✅ 25-39% referrals

### **Needs Improvement (Red Flags):**
- ❌ < 75% completion
- ❌ < 60% practice adoption
- ❌ < 60% 6-month retention
- ❌ < 80% harm prevention
- ❌ < 70% community active
- ❌ < 50 NPS
- ❌ < 4.0 CSAT
- ❌ < 25% referrals

---

## How We Report Success (Transparently)

### **Public Reporting (Website, Marketing):**
- Completion rate
- Student satisfaction (CSAT)
- NPS score
- Success stories (with permission)

### **Student Reporting (Dashboard for Current Students):**
- Their progress vs cohort average
- Frameworks they're using
- Community contributions
- Peer connections made

### **Team Reporting (Internal Dashboard):**
- All metrics above
- Red flags and alerts
- Cohort comparisons
- Trends over time

### **Investor/Stakeholder Reporting (Quarterly):**
- Revenue and growth
- LTV:CAC ratio
- Retention and NPS
- Market position

---

## File Metadata

**File Type:** Shared Knowledge for AI Agents
**Update Frequency:** Quarterly (metrics evolve as we learn)
**Owner:** Data Analyst + Chief Learning Strategist
**Related Files:**
- `strategic-objectives.md` (what we're optimising for)
- `quality-standards.md` (what excellence looks like)
- `student-personas.md` (who we're measuring)
- `brand-voice.md` (how we communicate results)

---

## Changelog

**v1.0 - October 25, 2025**
- Initial version created
- Five-tier metrics hierarchy established (Business, Transformation, Community, Satisfaction, Growth)
- 11 core metrics defined with elite targets
- Agent-specific metric usage documented
- Success tiers defined (Elite, Good, Needs Improvement)
- Transparent reporting approach defined

---

**END OF SUCCESS METRICS GUIDE**
